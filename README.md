# **Urban Mobility Data Explorer**


A full-stack web application and custom ETL pipeline designed to process, analyze, and visualize New York City Yellow Taxi trip data (TLC Dataset). This project features a Python/Flask backend, custom sorting and grouping algorithms, and an interactive frontend dashboard.

**Key Features**
- **Custom ETL Pipeline:** Extracts raw data, transforms and cleans anomalies (e.g., impossible speeds, extreme fares), and loads it into a relational SQLite database.

- **Data Conversion:** Includes utilities to convert massive CSV datasets into highly efficient Parquet formats before processing.

- **Algorithmic Processing:** Implements custom sorting (Bubble sort/Manual sorting) and grouping algorithms built from scratch, bypassing standard SQL operations for specific API endpoints.

- **Interactive Dashboard:** A responsive, dark-mode-enabled UI that visualizes borough demand, time-of-day efficiency, and data quality metrics using Chart.js.

- **RESTful API:** A Flask backend serving pre-calculated KPIs, spatial metadata, and paginated raw data.

## Project structure

`enterprise_web_dev_UrbanMobilityDataExplorer/
│
├── frontend/                   # Frontend UI and logic
│   ├── index.html              # Main dashboard view
│   ├── styles.css              # Custom styling & Dark Mode
│   ├── api.js                  # Frontend API service layer
│   └── dashboard.js            # Chart rendering and DOM manipulation
│
├── scripts/                    # Data processing and ETL pipeline
│   ├── convert_parquet.py      # Converts raw CSV to Parquet format
│   ├── etl_pipeline.py         # Main Extract, Transform, Load script
│   ├── init_db.py              # SQLite schema initialization
│   └── convert_spacial         # convert taxi zone metadata into a JSON file
│
├── data/                                   # Raw data folder 
│   ├── yellow_tripdata_2019-01.csv         # must be added by the user 
|   ├── yellow_tripdata_2019-01.parquet     # generated by a script 
|   ├── taxi_zones/                         # taxi zones spacial metadata
│   └── taxi_zone_lookup.csv
│
├── output/                     # Generated logs and outputs
|   ├── taxi_zones.json         # taxi zones spatial data in a JSON format 
│   └── suspicious_records.log  # Log of data rejected during ETL
│
├── app.py                      # Flask Application entry point
├── algorithms.py               # Custom sorting and grouping algorithms
├── requirements.txt            # Python dependencies
└── .gitignore                  # Git configuration`

## Data Cleaning & Preprocessing

Real-world taxi data contains significant noise and anomalies. Our Extract, Transform, Load (ETL) pipeline (`scripts/etl_pipeline.py`) automatically sanitizes the dataset before loading it into the SQLite database. 

The following strict data quality filters are applied to isolate and drop invalid or physically impossible trips:

* **Fare Outliers (Price Gouging):** Rejects trips that cost more than $50 but traveled less than 0.5 miles.
  
* **Impossible Short-Distance Speeds:** Rejects trips shorter than 1.0 mile with an average speed greater than 30 mph.
  
* **Zero Distance / High Fare:** Rejects trips that traveled 0.1 miles or less but cost more than $10.00.
  
* **Negative or Zero Fares:** Drops records where the total amount charged is $0 or less.
  
* **Invalid Durations:** Filters out trips with negative/zero time durations, or trips lasting longer than 12 hours (43,200 seconds).
  
* **Extreme Speeds:** Rejects any trip averaging over 100 mph or showing a negative speed.
  
* **Unknown Zones:** Rejects trips where the Pickup or Dropoff `LocationID` does not exist in the official TLC `taxi_zone_lookup.csv` directory.
  
* **Missing Value Handling:** Fills missing `congestion_surcharge` values with $0.00 and safely handles division-by-zero errors during speed calculations.

*Note: Rejected records are not deleted entirely; they are safely exported to `output/suspicious_records.log` alongside their specific rejection reason for further data quality auditing.*


# Prerequisites
To run this application, you will need:

- Python 3.8+ installed on your machine.

- The raw NYC TLC Taxi dataset (yellow_tripdata_2019-01.csv) and the taxi_zone_lookup.csv file.


# Setup & Installation Instructions
Follow these steps to configure the environment, process the data, and start the application.

**1. Clone the Repository**

    git clone <your-github-repo-url>
    
    cd enterprise_web_dev_UrbanMobilityDataExplorer

2. **Set Up the Virtual Environment**

It is highly recommended to run this project inside a virtual environment to avoid dependency conflicts.

- For Windows:
    
      python -m venv venv
      .\venv\Scripts\activate


- For macOS/Linux:

      python3 -m venv venv
      source venv/bin/activate


***3. Install Dependencies**
Install the required Python packages (Flask, Pandas, PyArrow/FastParquet, etc.):

    pip install -r requirements.txt


**4. Add the Raw Data**

A folder named data is in the root directory. Place your raw .csv files inside this folder:

data/yellow_tripdata_2019-01.csv

data/taxi_zone_lookup.csv

# Running the Application

**Step 1: Convert the CSV to Parquet**

To optimize the ETL process and handle the massive TLC dataset efficiently, first convert the raw .csv file into a compressed .parquet file using the provided script:

    python scripts/convert_parquet.py
    
(This will generate yellow_tripdata_2019-01.parquet inside your data/ folder).

**Step 2: Run the ETL Pipeline**

Initialize the database and process the newly created Parquet data. This script cleans the data, generates the SQLite database (database.db), and creates a log of rejected anomalous records.

    python scripts/etl_pipeline.py

**Step 3: Start the Flask Backend Server**

Once the database is populated, start the REST API:

    python app.py

The server will start running on **http://127.0.0.1:5000**. You will see a printout of all available API endpoints in your terminal.


**Step 4: Launch the Dashboard**

The frontend is completely decoupled from the Flask backend. To view the dashboard:

- Open the frontend/ folder.

- Double-click index.html to open it directly in your web browser (or use a tool like VS Code Live Server).

- The dashboard will automatically fetch data from the local Flask API.

# Key API Endpoints

**Utilities & Metadata**

* `GET /api/health` - API status and DB connection check
  
* `GET /api/zones` - Spatial metadata mapping (LocationIDs to Boroughs)

**Dashboard Statistics**

* `GET /api/stats/summary` - Total trips and average fare

* `GET /api/stats/charts/boroughs` - Aggregate trips per borough
  
* `GET /api/stats/charts/efficiency` - Average speed by time of day
  
* `GET /api/stats/quality` - Data quality metrics and rejected records

**Raw Data & Analytics**

* `GET /api/trips` - Raw trip records (paginated)
  
* `GET /api/analytics/summary` - Revenue, duration, and peak hours

**Custom Algorithms (Project Requirements)**

* `GET /api/trips/custom-sort` - Custom manual sorting
  
* `GET /api/trips/top-expensive` - Custom top-N selection algorithm
  
* `GET /api/analytics/borough-custom` - Custom grouping and aggregation


## Technology Stack

* **Frontend:** HTML5, CSS3 (Dark Mode Supported), Vanilla JavaScript, Chart.js
  
* **Backend:** Python, Flask, Flask-CORS
  
* **Data Engineering:** Pandas, FastParquet/PyArrow (Parquet conversion)
  
* **Database:** SQLite3
  
* **Architecture:** RESTful API, Custom ETL Pipeline


## Acknowledgments

* **Dataset:** The trip data and zone lookup tables are provided by the [NYC Taxi and Limousine Commission (TLC)](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page).


## Contributors 

Bendou Janna Vitalina Soeur 

Kamy Uwambaye

Tumba II Zikoranachukwudi M Kongolo


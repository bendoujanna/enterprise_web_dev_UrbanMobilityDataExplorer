# **Urban Mobility Data Explorer**


A full-stack web application and custom ETL pipeline designed to process, analyze, and visualize New York City Yellow Taxi trip data (TLC Dataset). This project features a Python/Flask backend, custom sorting and grouping algorithms, and an interactive frontend dashboard.

**Key Features**
- **Custom ETL Pipeline:** Extracts raw data, transforms and cleans anomalies (e.g., impossible speeds, extreme fares), and loads it into a relational SQLite database.

- **Data Conversion:** Includes utilities to convert massive CSV datasets into highly efficient Parquet formats before processing.

- **Algorithmic Processing:** Implements custom sorting (Bubble sort/Manual sorting) and grouping algorithms built from scratch, bypassing standard SQL operations for specific API endpoints.

- **Interactive Dashboard:** A responsive, dark-mode-enabled UI that visualizes borough demand, time-of-day efficiency, and data quality metrics using Chart.js.

- **RESTful API:** A Flask backend serving pre-calculated KPIs, spatial metadata, and paginated raw data.

## Project structure

```
enterprise_web_dev_UrbanMobilityDataExplorer/
│
├── frontend/                   # Frontend UI and logic
│   ├── index.html              # Main dashboard view
│   ├── styles.css              # Custom styling & Dark Mode
│   ├── api.js                  # Frontend API service layer
│   └── dashboard.js            # Chart rendering and DOM manipulation
│
├── scripts/                    # Data processing and ETL pipeline
│   ├── convert_parquet.py      # Converts raw CSV to Parquet format
│   ├── etl_pipeline.py         # Main Extract, Transform, Load script
│   ├── init_db.py              # SQLite schema initialization
│   └── convert_spacial         # convert taxi zone metadata into a JSON file
│
├── data/                                   # Raw data folder 
│   ├── yellow_tripdata_2019-01.csv         # must be added by the user 
|   ├── yellow_tripdata_2019-01.parquet     # generated by a script 
|   ├── taxi_zones/                         # taxi zones spacial metadata
│   └── taxi_zone_lookup.csv
├── docs/
|   ├── api_screenshots/                    # api
|   ├── database_screenshots/               # database          
│   └── architecture diagram/
|       └── ERD.png                         # entity relationships diagram
│
├── output/                     # Generated logs and outputs
|   ├── taxi_zones.json         # taxi zones spatial data in a JSON format 
│   └── suspicious_records.log  # Log of data rejected during ETL
│
├── app.py                      # Flask Application entry point
├── algorithms.py               # Custom sorting and grouping algorithms
├── requirements.txt            # Python dependencies
└── .gitignore                  # Git configuration
```

## Data Cleaning & Preprocessing

Real-world taxi data contains significant noise and anomalies. Our Extract, Transform, Load (ETL) pipeline (`scripts/etl_pipeline.py`) automatically sanitizes the dataset before loading it into the SQLite database. 

### 1. Data Integration & Normalization

* **Data Integration:** The pipeline systematically loads the highly compressed `.parquet` trip data and programmatically associates it with spatial metadata from `taxi_zone_lookup.csv` via relational mapping.
  
* **Normalization:** Standardizes all temporal data (`tpep_pickup_datetime`, `tpep_dropoff_datetime`) into strictly formatted datetime objects, sanitizes numeric fields (e.g., handling missing `congestion_surcharge` values by filling with `$0.00`), and aligns categorical identifiers for database storage.

###  2. Data Integrity (Anomaly Filtering)

The following strict data quality filters are applied to isolate and drop invalid or physically impossible trips:

* **Fare Outliers (Price Gouging):** Rejects trips that cost more than $50 but traveled less than 0.5 miles.
  
* **Impossible Short-Distance Speeds:** Rejects trips shorter than 1.0 mile with an average speed greater than 30 mph.
  
* **Zero Distance / High Fare:** Rejects trips that traveled 0.1 miles or less but cost more than $10.00.
  
* **Negative or Zero Fares:** Drops records where the total amount charged is $0 or less.
  
* **Invalid Durations:** Filters out trips with negative/zero time durations, or trips lasting longer than 12 hours (43,200 seconds).
  
* **Extreme Speeds:** Rejects any trip averaging over 100 mph or showing a negative speed.
  
* **Unknown Zones:** Rejects trips where the Pickup or Dropoff `LocationID` does not exist in the official TLC `taxi_zone_lookup.csv` directory.
  
* **Missing Value Handling:** Fills missing `congestion_surcharge` values with $0.00 and safely handles division-by-zero errors during speed calculations.


### 3. Feature Engineering

To provide deeper insights into urban movement and economics, we engineered **three derived features** from the raw columns:

1. **`trip_duration_seconds`**: Calculated by subtracting the pickup timestamp from the dropoff timestamp. *Justification:* Raw timestamps alone cannot easily be aggregated; this metric is essential for calculating traffic delays and route efficiency.
  
2. **`average_speed_mph`**: Calculated by dividing `trip_distance` by the derived duration. *Justification:* Acts as the primary indicator of urban congestion and physical outliers (detecting impossible speeds). Includes safety checks for division-by-zero errors.
  
3. **`time_of_day`**: Categorically bins the pickup hour into distinct periods (Morning, Afternoon, Evening, Night). *Justification:* Allows the dashboard to perform macro-level temporal analysis of demand patterns without getting bogged down by minute-by-minute variance.

### 4. Transparency and Auditing

Rejected records are not deleted entirely; they are safely exported to `output/suspicious_records.log` alongside their specific rejection reason for further data quality auditing.

## Database Architecture

The application utilizes **SQLite3** for its lightweight, serverless, and highly portable nature, making it perfect for rapid deployment and academic review. The relational schema is initialized via `scripts/init_db.py` and consists of three interconnected tables:

* **`trips` (Fact Table):** The core table storing all cleaned NYC taxi records. It contains numerical metrics (fares, distances, times) and acts as the central hub for the API queries.
  
* **`zones` (Dimension Table):** A spatial lookup table mapping TLC `LocationID`s to their human-readable `Borough` and `Zone` names.
  
* **`vendors` (Dimension Table):** A static reference table mapping `VendorID`s to their corporate entities (e.g., Creative Mobile Technologies, Curb Mobility).

### Query Optimization

To ensure the Flask API remains highly responsive when aggregating millions of rows for the frontend dashboard, the database schema includes targeted indexing:
* `idx_pickup` & `idx_dropoff`: Accelerates spatial filtering and geographic aggregations.
  
* `idx_date`: Speeds up time-series analytics (e.g., calculating average speeds by time of day).

  ## Custom Algorithmic Processing

To demonstrate a deep understanding of core data structures and computational efficiency, this project purposefully bypasses standard database-level operations (such as SQL `ORDER BY` or `GROUP BY`) for specific analytical API endpoints. Instead, the data is fetched and processed in-memory using custom Python algorithms located in `algorithms.py`.

* **Manual Sorting Algorithms:** Implemented custom sorting logic (e.g., Bubble Sort) to dynamically order trip arrays by Fare, Distance, or Speed. This allows the backend to handle complex, multi-variable sorting requirements before passing the JSON payload to the frontend.
  
* **In-Memory Grouping & Aggregation:** Utilizes hash maps (Python dictionaries) to manually iterate through records, grouping trips by spatial zones and calculating aggregate metrics (like total revenue or average speed) without relying on native SQL aggregate functions.
  
* **Top-N Selection:** Implements a custom iterative algorithm to scan the dataset and extract the top-most expensive trips, ensuring optimal memory management when handling large collections of data.

# Key API Endpoints

**Utilities & Metadata**

* `GET /api/health` - API status and DB connection check
  
* `GET /api/zones` - Spatial metadata mapping (LocationIDs to Boroughs)

**Dashboard Statistics**

* `GET /api/stats/summary` - Total trips and average fare

* `GET /api/stats/charts/boroughs` - Aggregate trips per borough
  
* `GET /api/stats/charts/efficiency` - Average speed by time of day
  
* `GET /api/stats/quality` - Data quality metrics and rejected records

**Raw Data & Analytics**

* `GET /api/trips` - Raw trip records (paginated)
  
* `GET /api/analytics/summary` - Revenue, duration, and peak hours

**Custom Algorithms (Project Requirements)**

* `GET /api/trips/custom-sort` - Custom manual sorting
  
* `GET /api/trips/top-expensive` - Custom top-N selection algorithm
  
* `GET /api/analytics/borough-custom` - Custom grouping and aggregation

## Frontend Dashboard Development

The user interface is a custom-built, responsive web dashboard engineered strictly with **HTML, CSS, and Vanilla JavaScript**, fulfilling all core project requirements without relying on heavy external JS frameworks.

 **1. Dynamic Interaction and Data Visualization**

The dashboard is built as a Single Page Application (SPA) that allows users to seamlessly interact with the dataset:

* **Visual Summaries:** The main view provides instant, dynamically calculated KPIs (Total Trips, Average Fare, Data Quality Score) and responsive charts (via Chart.js) visualizing Borough Demand and Average Speed over time.
  
* **Detail Views:** Users can toggle between the main dashboard, an Advanced Analytics view (revenue/duration metrics and peak hours), and a dedicated Data Quality breakdown.

**2. Advanced Filtering and Sorting**

The application features built-in controls that trigger our custom backend algorithms to manipulate the raw data table:

* **Location Filtering:** Users can instantly filter the raw trip records by specific geographical zones (e.g., Manhattan, Brooklyn, Queens, Bronx, EWR, Staten Island).
  
* **Custom Sorting:** Users can reorder the detailed view on the fly, sorting trips by **Fare** (`total_amount`), **Distance** (`trip_distance`), or **Speed** (MPH).

**3. Responsive and Accessible UI**

* Features a fully responsive CSS Flexbox/Grid architecture with media queries that safely stack components and allow horizontal table scrolling on mobile devices.
  
* Includes a dynamic Dark/Light mode toggle that updates CSS root variables and chart themes for better accessibility.


# Prerequisites

To run this application, you will need:

- Python 3.8+ installed on your machine.

- The raw NYC TLC Taxi dataset (yellow_tripdata_2019-01.csv) and the taxi_zone_lookup.csv file.


# Setup & Installation Instructions
Follow these steps to configure the environment, process the data, and start the application.

**1. Clone the Repository**

    git clone <your-github-repo-url>
    
    cd enterprise_web_dev_UrbanMobilityDataExplorer

2. **Set Up the Virtual Environment**

It is highly recommended to run this project inside a virtual environment to avoid dependency conflicts.

- For Windows:
    
      python -m venv venv
      .\venv\Scripts\activate


- For macOS/Linux:

      python3 -m venv venv
      source venv/bin/activate


***3. Install Dependencies**
Install the required Python packages (Flask, Pandas, PyArrow/FastParquet, etc.):

    pip install -r requirements.txt


**4. Add the Raw Data**

A folder named data is in the root directory. Place your raw .csv files inside this folder:

data/yellow_tripdata_2019-01.csv

data/taxi_zone_lookup.csv

# Running the Application

**Step 1: Convert the CSV to Parquet**

To optimize the ETL process and handle the massive TLC dataset efficiently, first convert the raw .csv file into a compressed .parquet file using the provided script:

    python scripts/convert_parquet.py
    
(This will generate yellow_tripdata_2019-01.parquet inside your data/ folder).

**Step 2: Run the ETL Pipeline**

Initialize the database and process the newly created Parquet data. This script cleans the data, generates the SQLite database (database.db), and creates a log of rejected anomalous records.

    python scripts/etl_pipeline.py

**Step 3: Start the Flask Backend Server**

Once the database is populated, start the REST API:

    python app.py

The server will start running on **http://127.0.0.1:5000**. You will see a printout of all available API endpoints in your terminal.


**Step 4: Launch the Dashboard**

The frontend is completely decoupled from the Flask backend. To view the dashboard:

- Open the frontend/ folder.

- Double-click index.html to open it directly in your web browser (or use a tool like VS Code Live Server).

- The dashboard will automatically fetch data from the local Flask API.



## Technology Stack

* **Frontend:** HTML5, CSS, Vanilla JavaScript, Chart.js
  
* **Backend:** Python, Flask, Flask-CORS
  
* **Data Engineering:** Pandas, FastParquet/PyArrow (Parquet conversion)
  
* **Database:** SQLite3
  
* **Architecture:** RESTful API, Custom ETL Pipeline


## Acknowledgments

* **Dataset:** The trip data and zone lookup tables are provided by the [NYC Taxi and Limousine Commission (TLC)](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page).

## video link

https://www.loom.com/share/4c8ab46842994590887dfbfc228cd273

## task sheet

https://docs.google.com/spreadsheets/d/1rM1W1w6Jr3aj3OJ7ED7FgRfbUuQBZ_Z0T-EkieM8g8A/edit?usp=sharing

## .zip 

[UrbanMobilityDataExplorer.zip](https://github.com/user-attachments/files/25428723/UrbanMobilityDataExplorer.zip)



## Contributors 

Bendou Janna Vitalina Soeur 

Kamy Uwambaye

Tumba II Zikoranachukwudi M Kongolo

